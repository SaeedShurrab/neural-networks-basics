{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Libraries Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions defination\n",
    "def activation(x,activation_type= 'sigmoid'):\n",
    "    \n",
    "    if activation_type not in ['sigmoid','tanh', 'relu','lrelu']:\n",
    "        raise ValueError(\" activation type must be in ['sigmoid','tanh', 'relu','lrelu']\")\n",
    "    \n",
    "    if activation_type == 'sigmoid':\n",
    "        return (1/(1+np.exp(-x)))\n",
    "    \n",
    "    elif activation_type == 'tanh':\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    elif activation_type == 'relu':\n",
    "        return np.maximum(0.0,x)\n",
    "    \n",
    "    elif activation_type == 'lrelu':\n",
    "        return np.maximum(0.01 * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation fumnctions derivatives\n",
    "def act_derivative(x, activation_type= 'sigmoid'):\n",
    "\n",
    "    if activation_type not in ['sigmoid','tanh', 'relu','lrelu']:\n",
    "        raise ValueError(\" activation type must be in ['sigmoid','tanh', 'relu','lrelu']\")    \n",
    "    \n",
    "    if activation_type == 'sigmoid':\n",
    "        return activation(x, activation_type= activation_type)\\\n",
    "        * (1- activation(x, activation_type=activation_type))\n",
    "    \n",
    "    elif activation_type == 'tanh':\n",
    "        return 1- (activation(x,activation_type= activation_type))**2\n",
    "    \n",
    "    elif activation_type == 'relu':\n",
    "        return np.where(x <= 0.0, 0.0,1.0)\n",
    "    \n",
    "    elif activation_type == 'lrelu':\n",
    "        return np.where(x <= 0.0, 0.01 * x,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random data generation for function\n",
    "def data_generator(num_features=10, num_examples=500,train_p=0.8):\n",
    "    \n",
    "    dataset = np.random.randint(0,20,size=(num_features,num_examples))\n",
    "    labels = np.random.randint(0,2,(1,num_examples))\n",
    "    \n",
    "    x_train = dataset[:,:int(num_examples * train_p)]\n",
    "    y_train = labels[:,:int(num_examples * train_p)]\n",
    "    x_test = dataset[:,int(num_examples * train_p) :]\n",
    "    y_test = labels[:,int(num_examples * train_p):]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Network Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random data generation\n",
    "x_train, y_train, x_test, y_test = data_generator(10,1000,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of training examples is: 750\n",
      "the number of testing examples is : 250\n",
      "the number input fetures is       : 10\n",
      "-----------------------------------------\n",
      "the training datset shape is : (10, 750)\n",
      "the training labels shape is : (1, 750)\n",
      "the testing datset shape is : (10, 250)\n",
      "the testing labels shape is : (1, 250)\n"
     ]
    }
   ],
   "source": [
    "print('the number of training examples is: '+ str(x_train.shape[1]))\n",
    "print('the number of testing examples is : '+ str(x_test.shape[1]))\n",
    "print('the number input fetures is       : '+ str(x_train.shape[0]))\n",
    "print('-----------------------------------------')\n",
    "print('the training datset shape is : ' + str(x_train.shape))\n",
    "print('the training labels shape is : ' + str(y_train.shape))\n",
    "print('the testing datset shape is : ' + str(x_test.shape))\n",
    "print('the testing labels shape is : ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters initiation\n",
    "def initialize_parameters(X, Y, hidden_size= 5):\n",
    "    input_size =  X.shape[0]\n",
    "    output_size = Y.shape[0]\n",
    "    \n",
    "    W1 = np.random.randn(hidden_size,input_size)*.01\n",
    "    b1 = np.zeros((hidden_size,1)) \n",
    "    W2 = np.random.randn(output_size,hidden_size)*.01\n",
    "    b2 = np.zeros((output_size,1))\n",
    "    \n",
    "    assert (W1.shape == (hidden_size, input_size))\n",
    "    assert (b1.shape == (hidden_size, 1))\n",
    "    assert (W2.shape == (output_size, hidden_size))\n",
    "    assert (b2.shape == (output_size, 1))\n",
    "    \n",
    "    parameters ={'W1':W1,\n",
    "                 'b1':b1,\n",
    "                 'W2':W2,\n",
    "                 'b2':b2} \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation function\n",
    "def forward_prop(X, Y, parameters, hidden_activation= 'tanh'):\n",
    "       \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']   \n",
    "       \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = activation(Z1,activation_type=hidden_activation)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = activation(Z2, activation_type= 'sigmoid')\n",
    "        \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    cost = np.squeeze((-1/m)*(np.dot(Y,np.log(A2).T)+ np.dot((1-Y),np.log(1-A2).T)))\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    cache = {'Z1':Z1,\n",
    "             'A1':A1,\n",
    "             'Z2':Z2,\n",
    "             'A2':A2}\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation function\n",
    "def backward_prop(X, Y, parameters,cache, hidden_activation = 'tanh'):\n",
    "        \n",
    "    m = x_train.shape[1]\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    Z1 =cache['Z1']\n",
    "    \n",
    "    dZ2 =  A2-y_train\n",
    "    dW2 = (1/m) * np.dot(dZ2,A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis= 1,keepdims=True)\n",
    "    \n",
    "    dZ1 = np.dot(W2.T,dZ2) * act_derivative(Z1, activation_type= hidden_activation)\n",
    "    dW1 = (1/m) * np.dot(dZ1,X.T)\n",
    "    db1 = np.sum(dZ1, axis= 1, keepdims= True)\n",
    "    \n",
    "    derivatives= {'dW2':dW2,\n",
    "                  'db2':db2,\n",
    "                  'dW1':dW1,\n",
    "                  'db1':db1}\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights optimization \n",
    "def optimize(parameters,derivatives, lr =0.01):\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    dW1 = derivatives['dW1']\n",
    "    db1 = derivatives['db1']\n",
    "    dW2 = derivatives['dW2']\n",
    "    db2 = derivatives['db2']\n",
    "    \n",
    "    parameters = {'W1': W1 - lr * dW1, \n",
    "                 'b1': b1 - lr * db1, \n",
    "                 'W2': W2 - lr * dW2, \n",
    "                 'b2': b2 - lr * db2}\n",
    "    \n",
    "    return parameters \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all function into a single function\n",
    "def model(X, Y, num_iterations= 10000, hidden_size = 5, hidden_activation='tanh',lr = 0.01 ,print_cost = True):\n",
    "    \n",
    "    parameters = initialize_parameters(X,Y, hidden_size= hidden_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        cost, cache = forward_prop(X,Y,parameters, hidden_activation= hidden_activation)\n",
    "        \n",
    "        derivatives = backward_prop(X,Y,parameters,cache,hidden_activation=hidden_activation)\n",
    "        \n",
    "        parameters = optimize(parameters,derivatives,lr=lr)\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "    print('\\nthe ultimate cost value is: ' + str(cost))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(X,Y,parameters, hidden_activation='tanh'):\n",
    "    \n",
    "    _ , cache = forward_prop(X, Y,parameters, hidden_activation = hidden_activation)\n",
    "    predictions = (cache['A2']>0.5)\n",
    "    print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693035\n",
      "\n",
      "the ultimate cost value is: 0.6863694428272646\n"
     ]
    }
   ],
   "source": [
    "parameters = model(x_train,y_train, num_iterations=1000,hidden_size=50,hidden_activation='tanh',lr=0.03, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99%\n"
     ]
    }
   ],
   "source": [
    "predict(x_train,y_train,parameters, hidden_activation = 'tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47%\n"
     ]
    }
   ],
   "source": [
    "predict(x_test,y_test,parameters, hidden_activation = 'tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
